# **Log Analyzer**

Log Analyzer — это инструмент для анализа логов Nginx. Он парсит файлы логов, вычисляет статистические метрики (например, среднее, медианное время ответа) и генерирует HTML-отчеты.

---

## **Содержание**

1. [Описание проекта](#описание-проекта)
2. [Требования](#требования)
3. [Установка](#установка)
4. [Использование](#использование)
5. [Генерация отчетов](#генерация-отчетов)
6. [Линтинг и форматирование кода](#линтинг-и-форматирование-кода)
7. [Сборка Docker-образа](#сборка-docker-образа)
8. [CI/CD](#cicd)
9. [Мониторинг и логирование](#мониторинг-и-логирование)

---

## **Описание проекта**

Log Analyzer:
- Находит последний лог-файл в указанной директории.
- Парсит логи для извлечения URL, времени запроса и других метрик.
- Вычисляет статистические данные, такие как:
  - Общее количество запросов.
  - Среднее, медианное и максимальное время ответа.
  - Процент времени запросов по отношению к общему времени.
- Генерирует HTML-отчет на основе собранных данных.
- Проверяет процент ошибок парсинга и завершается с ошибкой, если превышает порог.
- Избегает повторной обработки уже обработанных логов.

---

## **Требования**

Для работы с проектом необходимы:
- Python 3.9+
- Poetry (для управления зависимостями)
- Docker (опционально, для контейнеризации)

---

## **Установка**

1. Клонируйте репозиторий:
   ```bash
   git clone https://github.com/your-repo/log-analyzer.git
   cd log-analyzer
   ```

2. Установите зависимости:
   ```bash
   poetry install
   ```

3. Активируйте виртуальное окружение:
   ```bash
   poetry shell
   ```

---

## **Использование**

### **Запуск анализа логов**
Для запуска анализа используйте команду:
```bash
poetry run python src/main.py --log-dir ./logs --report-dir ./reports
```

#### Аргументы командной строки:
- `--log-dir`: Путь к директории с логами (по умолчанию: `./logs`).
- `--report-dir`: Путь к директории для сохранения отчетов (по умолчанию: `./reports`).
- `--config`: Путь к файлу конфигурации (опционально).

Пример использования файла конфигурации (`config.json`):
```json
{
  "LOG_DIR": "./custom_logs",
  "REPORT_DIR": "./custom_reports",
  "REPORT_SIZE": 1000,
  "PARSING_ERROR_THRESHOLD": 0.5
}
```

### **Формат логов**
Поддерживаются следующие форматы:
- Логи Nginx в формате `"GET /index.html HTTP/1.1" ... 0.123`.
- Логи с ошибками в формате `"0" ... 0.000`.

---

## **Генерация отчетов**

Отчеты сохраняются в формате HTML в директории `REPORT_DIR`. Пример содержимого отчета:

```html
<!DOCTYPE html>
<html>
<head>
    <title>Report</title>
</head>
<body>
    <div id="data">
        [
            {"url": "/index.html", "count": 100, "time_sum": 12.345, ...},
            {"url": "/api/data", "count": 50, "time_sum": 8.123, ...}
        ]
    </div>
</body>
</html>
```

Если отчет для данного лога уже существует, он не будет пересоздан.

---

## **Линтинг и форматирование кода**

Проект использует следующие инструменты для контроля качества кода:
- **Black**: Автоматическое форматирование кода.
- **Flake8**: Проверка соответствия PEP 8.
- **Mypy**: Статическая проверка типов.
- **Bandit**: Анализ безопасности кода.

Для запуска всех проверок используйте:
```bash
make lint
```

---

## **Сборка Docker-образа**

1. Соберите Docker-образ:
   ```bash
   docker build -t log-analyzer .
   ```

2. Запустите контейнер:
   ```bash
   docker run --rm \
     -v $(pwd)/logs:/logs \
     -v $(pwd)/reports:/reports \
     log-analyzer
   ```

---

## **CI/CD**

Проект использует GitHub Actions для автоматизации:
- Запуска тестов.
- Линтинга кода.
- Проверки безопасности.

Конфигурация находится в `.github/workflows/ci.yml`.

---

## **Мониторинг и логирование**

1. **Логирование**:
   - Скрипт пишет структурированные логи в JSON через `structlog`.
   - Если путь до логфайла указан в конфиге, логи записываются в файл. Иначе — в `stdout`.

2. **Обработка ошибок парсинга**:
   - Если процент непарсированных строк превышает порог (`PARSING_ERROR_THRESHOLD`), скрипт завершается с ошибкой.

3. **Повторная обработка**:
   - Если отчет для лога уже существует, скрипт пропускает его.
